{"cells":[{"cell_type":"markdown","metadata":{"id":"MDkEHVlsAKyw"},"source":["# Intergrated Gradients of the nl-BERT-large Model Prior to Fine-tuning\n","In this notebook we:\n","\n","1. Employ the model without any fine-tuning\n","2. extract Intergrated Gradients (IG) values for the model to note which features (here tokens) contributes to the classification of either \"female\" or \"male\" artists in the pre-fine-tuning state of the nl-BERT-large model.\n","\n","The output os this can then be compared to those of the model post fine-tuning in the `IG_DIFFERENTIALS.ipynb`.\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18063,"status":"ok","timestamp":1703070036698,"user":{"displayName":"Laura Wulff Paaby","userId":"02296487503463549213"},"user_tz":-60},"id":"BgtLzcP_P7Jg","outputId":"d8ebe9d0-dda0-405e-e76f-a185903c1e6f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30090,"status":"ok","timestamp":1703070066784,"user":{"displayName":"Laura Wulff Paaby","userId":"02296487503463549213"},"user_tz":-60},"id":"LLOCXWJ1AdJi","outputId":"6fe6b014-12dc-4820-8dc4-d258604e4361"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m925.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m533.5/533.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m998.1/998.1 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["#### Install packages:\n","!pip install -q transformers transformers-interpret datasets evaluate tensorflow spacy spacy_langdetect shap matplotlib\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":30347,"status":"ok","timestamp":1703070097124,"user":{"displayName":"Laura Wulff Paaby","userId":"02296487503463549213"},"user_tz":-60},"id":"hIXLtLjCApF_"},"outputs":[],"source":["### Importing packages:\n","import pandas as pd\n","import torch\n","from datasets import Dataset, DatasetDict, load_metric\n","from transformers import AutoTokenizer, AutoModelForPreTraining, Trainer, TrainingArguments, AutoModelForSequenceClassification, BertTokenizer\n","from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n","from transformers import EarlyStoppingCallback\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"aWN4PzhAAraN"},"source":["Loading the pretrained model:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11901,"status":"ok","timestamp":1703022794124,"user":{"displayName":"Laura Wulff Paaby","userId":"02296487503463549213"},"user_tz":-60},"id":"1oWgHdLPAt87","outputId":"f995c61f-2b58-469d-f4e5-760bf5c8f23a"},"outputs":[],"source":["from transformers import BertConfig\n","\n","# the pretrained model:\n","original_model = \"NbAiLab/nb-bert-large\"\n","\n","model = AutoModelForSequenceClassification.from_pretrained(original_model)\n","\n","# Load the tokenizer from the original pre-trained model\n","tokenizer = AutoTokenizer.from_pretrained(original_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QSyFfuc0A9Wc"},"outputs":[],"source":["df = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/NLP_EX_COLAB/eval_outputs/NBL_df_classification_report.csv')\n","texts = df['Text'].tolist() # extracting all texts to a list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kQSDdUCvA_dT"},"outputs":[],"source":["from transformers_interpret import SequenceClassificationExplainer\n","\n","# Create a sequence classification explainer\n","explainer = SequenceClassificationExplainer(\n","    model=model,\n","    tokenizer=tokenizer\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"aAKxCshSBGdy"},"source":["### Extracting and saving the IG values 5 batches at a time\n","This is done due to limited GPU capacity. Everything beyond 5 batches, crashes the run.\n","Thus, the explainer function is used continously at each 5 batch and the output saved for each round."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c2IBnWd7BKPv"},"outputs":[],"source":["import os\n","import json\n","\n","# Function to divide texts into batches\n","def batchify(texts, batch_size):\n","    return [texts[i:i + batch_size] for i in range(0, len(texts), batch_size)]\n","\n","# Define batch size and divide texts into batches\n","batch_size = 5\n","text_batches = batchify(texts, batch_size)\n","\n","# File to track processed batches\n","processed_batches_file = '/content/drive/MyDrive/Colab_Notebooks/NLP_EX_COLAB/IG/pretrained_IG_output/processed_batches.txt'\n","\n","# Function to save IG values\n","def save_shap_values(batch_index, shap_values):\n","    with open(f'/content/drive/MyDrive/Colab_Notebooks/NLP_EX_COLAB/IG/pretrained_IG_output/shap_values_batch_{batch_index}.json', 'w') as f:\n","        json.dump(shap_values, f)\n","\n","# Function to get the last processed batch index\n","def get_last_processed_batch():\n","    if os.path.exists(processed_batches_file):\n","        with open(processed_batches_file, 'r') as f:\n","            return int(f.read().strip())\n","    return 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FByEbH7gCU3v"},"outputs":[],"source":["# Start processing from the next batch after the last processed batch\n","last_processed_batch = get_last_processed_batch()\n","\n","for batch_index, batch in enumerate(text_batches):\n","    if batch_index <= last_processed_batch:\n","        continue\n","\n","    # Process batch\n","    batch_shap_values = [explainer(text) for text in batch]\n","\n","    # Save IG values\n","    save_shap_values(batch_index, batch_shap_values)\n","\n","    # Update processed batches file\n","    with open(processed_batches_file, 'w') as f:\n","        f.write(str(batch_index))\n","\n","    # Clear memory if needed\n","    del batch_shap_values\n","    torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CmxP-BL62qHD"},"outputs":[],"source":["#### GET BATCH 0\n","text0 = df['Text'][0:5].tolist()\n","batch_shap_values = [explainer(text) for text in text0]\n","save_shap_values(0, batch_shap_values)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPZ9+t9M6bvwlHxKSnm1oX9","mount_file_id":"1qZtoZwV_zq1t1tG80_R7Zpzsqmillb4T","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
