{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article Scraping Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in URLs\n",
    "path_to_urls = 'webscraper_eb_urls.csv'\n",
    "df = pd.read_csv(path_to_urls, delimiter=';', header=None)\n",
    "df.columns = ['Title', 'URL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append all URLs to a list \n",
    "urls = []\n",
    "for url in df['URL']:\n",
    "    urls.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_eb(url_input):\n",
    "    \"\"\"\n",
    "    Function for scraping EB articles from a list of URLs. \n",
    "    \"\"\"\n",
    "    \n",
    "        # Loop through each URL\n",
    "    with open('/Users/emmaolsen/NLP-EXAM-23/scraping/unsuccessful_scrapes.txt', 'w') as file:\n",
    "        for index, url in enumerate(url_input, start=0):\n",
    "            try:\n",
    "                # Send a GET request\n",
    "                response = requests.get(url)\n",
    "                # Check if the request was successful\n",
    "                response.raise_for_status()\n",
    "\n",
    "                # Parse the HTML content\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                \n",
    "                # Find the <span> element with the data-timestamp attribute\n",
    "                timestamp_element = soup.find('span', {'data-timestamp': True})\n",
    "\n",
    "                # Extract the timestamp value\n",
    "                timestamp = timestamp_element['data-timestamp']\n",
    "                            \n",
    "                # Extract elements by class name\n",
    "                title = soup.find('h1', class_='art-title').get_text(strip=True)\n",
    "                subtitle = soup.find('h2', class_='art-subtitle').get_text(strip=True)\n",
    "                reviewer = soup.find('a', class_='fontweight-bold').get_text(strip=True)\n",
    "                bodytext = soup.find('div', id='fnBodytextTracking').get_text(strip=True)\n",
    "                \n",
    "                \n",
    "                # Find all elements with the class \"icon-svg eb-rating\"\n",
    "                eb_elements = soup.find_all(class_=\"icon-svg eb-rating\")\n",
    "                # Count the total number of instances\n",
    "                eb_rating = len(eb_elements)\n",
    "                \n",
    "            \n",
    "                user_elements = soup.find_all(class_=\"icon-svg user-rating\")\n",
    "                # Initialize a variable to keep track of the star count\n",
    "                user_star_count = 0\n",
    "                # Iterate through the elements and count stars based on the use xlink:href attribute\n",
    "                for element in user_elements:\n",
    "                    xlink_href = element.find('use')['xlink:href']\n",
    "                    if xlink_href == '#star-solid':\n",
    "                        user_star_count += 1\n",
    "                    elif xlink_href == '#star-half-solid':\n",
    "                        user_star_count += 0.5\n",
    "            \n",
    "                eb_elements = soup.find_all(class_=\"icon-svg eb-rating\")\n",
    "                # Initialize a variable to keep track of the star count\n",
    "                eb_star_count = 0\n",
    "                # Iterate through the elements and count stars based on the use xlink:href attribute\n",
    "                for element in eb_elements:\n",
    "                    xlink_href = element.find('use')['xlink:href']\n",
    "                    if xlink_href == '#star-solid':\n",
    "                        eb_star_count += 1\n",
    "                    elif xlink_href == '#star-half-solid':\n",
    "                        eb_star_count += 0.5\n",
    "            \n",
    "            \n",
    "                # Append to the list as a dictionary\n",
    "                data.append({\n",
    "                    'Url': url,\n",
    "                    'Date': timestamp,\n",
    "                    'Title': title,\n",
    "                    'Subtitle': subtitle,\n",
    "                    'Reviewer': reviewer,\n",
    "                    'EB_Rating': eb_star_count-6,\n",
    "                    'US_Rating': user_star_count-6,\n",
    "                    'BodyText': bodytext\n",
    "                })\n",
    "\n",
    "            except Exception:\n",
    "                # Write the unsuccessful URL to the text file\n",
    "                file.write(url + '\\n')\n",
    "\n",
    "    # Convert the list of dictionaries to a pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    df['Date'] = df['Date'].str[:10]\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply scraping function to the list of urls\n",
    "eb_df = scrape_eb(urls)\n",
    "# turn all columns into strings\n",
    "eb_df = eb_df.astype(str)\n",
    "# remove rows where all values except URL are the same\n",
    "eb_df = eb_df.drop_duplicates(subset=['Date', 'Title', 'Subtitle', 'Reviewer', 'EB_Rating', 'US_Rating', 'BodyText'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "eb_df.to_csv('/Users/emmaolsen/NLP-EXAM-23/data/full_scraped_articles.csv')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
